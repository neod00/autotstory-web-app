#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
(ê¸°í›„ë³€í™”, ì¬í…Œí¬, ì •ë³´ê¸°ìˆ , ì£¼ì‹, ê²½ì œ ì „ë¬¸)
"""

import os
import time
import re
import openai
import requests
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

def test_dynamic_date_detection():
    """ë™ì  ë‚ ì§œ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“… 1. ë™ì  ë‚ ì§œ ê°ì§€ í…ŒìŠ¤íŠ¸")
    print("-" * 30)
    
    now = datetime.now()
    current_year = now.year
    current_month = now.month
    current_day = now.day
    current_date_str = now.strftime("%Yë…„ %mì›” %dì¼")
    
    print(f"âœ… í˜„ì¬ ì‹œì : {current_date_str}")
    print(f"   ì—°ë„: {current_year}")
    print(f"   ì›”: {current_month}")
    print(f"   ì¼: {current_day}")
    
    # ê³„ì ˆ ê°ì§€
    if current_month in [12, 1, 2]:
        season = "ê²¨ìš¸"
    elif current_month in [3, 4, 5]:
        season = "ë´„"
    elif current_month in [6, 7, 8]:
        season = "ì—¬ë¦„"
    else:
        season = "ê°€ì„"
    
    print(f"   í˜„ì¬ ê³„ì ˆ: {season}")
    
    return {
        "year": current_year,
        "month": current_month,
        "season": season,
        "date_str": current_date_str
    }

def test_specialized_queries(date_info):
    """ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” ê²€ìƒ‰ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ” 2. ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    current_year = date_info["year"]
    current_month = date_info["month"]
    season = date_info["season"]
    
    # ì „ë¬¸ ë¶„ì•¼ë³„ ì¿¼ë¦¬ ìƒì„±
    specialized_queries = [
        f"{current_year}ë…„ {current_month}ì›” ê¸°í›„ë³€í™” ì •ì±… ë™í–¥",
        f"{current_year} ë¯¸êµ­ì£¼ì‹ ì‹œì¥ ì „ë§",
        f"{current_year}ë…„ ì¬í…Œí¬ íŠ¸ë Œë“œ",
        f"{current_year} ì •ë³´ê¸°ìˆ  í˜ì‹ ",
        f"{current_year}ë…„ {season} ê²½ì œ ì „ë§",
        f"{current_year} ì§€ì†ê°€ëŠ¥ì„± ê¸°ìˆ ",
        f"{current_year}ë…„ êµ­ë‚´ì£¼ì‹ ë¶„ì„",
        f"{current_year} í™˜ê²½ì •ì±… ë³€í™”",
        f"{current_year}ë…„ ê¸€ë¡œë²Œ ê²½ì œ ì´ìŠˆ",
        f"{current_year} ESG íˆ¬ì ë™í–¥"
    ]
    
    print(f"âœ… ìƒì„±ëœ ì „ë¬¸ ì¿¼ë¦¬ ({len(specialized_queries)}ê°œ):")
    for i, query in enumerate(specialized_queries, 1):
        print(f"   {i:2d}. \"{query}\"")
    
    return specialized_queries

def test_specialized_web_search_mock(queries):
    """ì „ë¬¸ ë¶„ì•¼ ì›¹ ê²€ìƒ‰ ëª¨ì˜ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸŒ 3. ì „ë¬¸ ë¶„ì•¼ ì›¹ ê²€ìƒ‰ ëª¨ì˜ í…ŒìŠ¤íŠ¸")
    print("-" * 35)
    
    # ì „ë¬¸ ë¶„ì•¼ë³„ í˜„ì‹¤ì ì¸ ëª¨ì˜ ë°ì´í„°
    current_year = datetime.now().year
    specialized_mock_results = {
        f"{current_year}ë…„ ê¸°í›„ë³€í™” ì •ì±…": f"""
        {current_year}ë…„ ìƒë°˜ê¸° ê¸°í›„ë³€í™” ëŒ€ì‘ ì •ì±…ì˜ í•µì‹¬ ë³€í™”ë¥¼ ì‚´í´ë³´ë©´,
        íƒ„ì†Œì¤‘ë¦½ ë¡œë“œë§µ ì‹¤í–‰, ì¬ìƒì—ë„ˆì§€ í™•ëŒ€ ì •ì±…, ê·¸ë¦°ë‰´ë”œ 2.0 ì¶”ì§„ì´ ì£¼ìš” ì´ìŠˆì…ë‹ˆë‹¤.
        íŠ¹íˆ íƒ„ì†Œë°°ì¶œê¶Œ ê±°ë˜ì œ ê°œì„ , ë…¹ìƒ‰ë¶„ë¥˜ì²´ê³„ ë„ì…, ESG ê³µì‹œ ì˜ë¬´í™”ê°€ ê¸°ì—…ë“¤ì—ê²Œ 
        í° ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìœ¼ë©°, ì¹œí™˜ê²½ ê¸°ìˆ  íˆ¬ì í™•ëŒ€ì™€ ì§€ì†ê°€ëŠ¥ ê¸ˆìœµ í™œì„±í™” ì •ì±…ë„ 
        ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ê¸°í›„í…Œí¬ ìŠ¤íƒ€íŠ¸ì—… ì§€ì›, ìˆ˜ì†Œê²½ì œ í™œì„±í™” ë°©ì•ˆë„ ì¤‘ìš” í‚¤ì›Œë“œì…ë‹ˆë‹¤.
        """,
        
        f"{current_year} ë¯¸êµ­ì£¼ì‹ ì‹œì¥": f"""
        {current_year}ë…„ ë¯¸êµ­ì£¼ì‹ ì‹œì¥ì€ ì¸í”Œë ˆì´ì…˜ ì•ˆì •í™”, ì—°ì¤€ ê¸ˆë¦¬ ì •ì±… ë³€í™”, 
        AI ê¸°ìˆ ì£¼ ê¸‰ì„±ì¥ì´ ì£¼ìš” ë™ë ¥ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        ì—”ë¹„ë””ì•„, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸, ì• í”Œ ë“± í…Œí¬ ëŒ€ê¸°ì—…ë“¤ì˜ ì‹¤ì ì´ ì‹œì¥ì„ ê²¬ì¸í•˜ë©°,
        ìƒì„±AI ê´€ë ¨ì£¼, ë°˜ë„ì²´ ì¥ë¹„ì£¼, í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì£¼ê°€ ê°•ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.
        ë°°ë‹¹ ê·€ì¡±ì£¼ íˆ¬ì ì „ëµ, ë‹¬ëŸ¬ ê°•ì„¸ ì˜í–¥ ë¶„ì„, ê²½ê¸°ì¹¨ì²´ ìš°ë ¤ ì™„í™”ë„ 
        íˆ¬ììë“¤ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬ì…ë‹ˆë‹¤.
        """,
        
        f"{current_year}ë…„ ì¬í…Œí¬ íŠ¸ë Œë“œ": f"""
        {current_year}ë…„ ì¬í…Œí¬ ì‹œì¥ì˜ ì£¼ìš” íŠ¸ë Œë“œëŠ” ê³ ê¸ˆë¦¬ ì§€ì†, ë¶€ë™ì‚° ì‹œì¥ ë³€í™”,
        ë””ì§€í„¸ ìì‚° ë‹¤ë³€í™”ì— ì´ˆì ì´ ë§ì¶°ì ¸ ìˆìŠµë‹ˆë‹¤.
        CMA í†µì¥ í™œìš©ë²•, êµ­ì±„ ETF íˆ¬ì, ë‹¬ëŸ¬ì˜ˆê¸ˆ ì „ëµì´ ì¸ê¸°ë¥¼ ëŒê³  ìˆìœ¼ë©°,
        ISA ê³„ì¢Œ ìµœì  í™œìš©, ì—°ê¸ˆì €ì¶• ì„¸ì•¡ê³µì œ ì „ëµ, ê°œì¸ì—°ê¸ˆ ìƒí’ˆ ë¹„êµë„ 
        í•µì‹¬ ê´€ì‹¬ì‚¬ì…ë‹ˆë‹¤. ê°€ìƒí™”í í˜„ë¬¼ ETF ì¶œì‹œ, ë¦¬ì¸  íˆ¬ì í™•ëŒ€, 
        í•´ì™¸ì£¼ì‹ íˆ¬ì ë‹¤ë³€í™” ì „ëµë„ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.
        """,
        
        f"{current_year} ì •ë³´ê¸°ìˆ  í˜ì‹ ": f"""
        {current_year}ë…„ IT í˜ì‹ ì˜ í•µì‹¬ì€ ìƒì„±AI ê¸°ìˆ  ë°œì „, í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì „í™˜,
        ë©”íƒ€ë²„ìŠ¤ ì‹¤ìš©í™”, ì–‘ìì»´í“¨íŒ… ìƒìš©í™” ì§„ì „ì— ìˆìŠµë‹ˆë‹¤.
        ChatGPT-4 ê³ ë„í™”, êµ¬ê¸€ ë°”ë“œ ê²½ìŸ, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ ì½”íŒŒì¼ëŸ¿ í™•ì‚°ì´ 
        ì—…ë¬´ í™˜ê²½ì„ í˜ì‹ í•˜ê³  ìˆìœ¼ë©°, ì—£ì§€ ì»´í“¨íŒ… í™•ì‚°, 5G ì „ìš©ë§ êµ¬ì¶•,
        ë¸”ë¡ì²´ì¸ ì‹¤ìš©í™”ë„ ê°€ì†í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ì´ë²„ë³´ì•ˆ ê°•í™”, ê°œì¸ì •ë³´ë³´í˜¸ 
        ê¸°ìˆ  ë°œì „, ë””ì§€í„¸ ì „í™˜ ê°€ì†í™”ë„ ì£¼ìš” ì´ìŠˆì…ë‹ˆë‹¤.
        """
    }
    
    collected_trends = []
    search_info = []
    
    for i, query in enumerate(queries[:4]):  # ì²˜ìŒ 4ê°œë§Œ í…ŒìŠ¤íŠ¸
        print(f"ğŸ” ê²€ìƒ‰ ì¤‘: \"{query}\"")
        time.sleep(0.5)  # ì‹œë®¬ë ˆì´ì…˜ ë”œë ˆì´
        
        # í‚¤ì— í¬í•¨ëœ ì¿¼ë¦¬ ì°¾ê¸°
        result_text = ""
        for key, value in specialized_mock_results.items():
            if any(word in query for word in key.split()):
                result_text = value.strip()
                break
        
        if not result_text and i < len(specialized_mock_results):
            result_text = list(specialized_mock_results.values())[i]
        
        if result_text:
            print(f"   âœ… ê²€ìƒ‰ ê²°ê³¼ íšë“ ({len(result_text)} ê¸€ì)")
            
            # ì „ë¬¸ ë¶„ì•¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            extracted = extract_specialized_topics(result_text, current_year)
            collected_trends.extend(extracted)
            
            search_info.append({
                "query": query,
                "result": result_text[:100] + "...",
                "extracted_topics": extracted
            })
            
            print(f"   ğŸ“ ì¶”ì¶œëœ ì „ë¬¸ ì£¼ì œ: {', '.join(extracted) if extracted else 'ì—†ìŒ'}")
        else:
            print(f"   âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    
    unique_trends = list(set(collected_trends))
    print(f"\nâœ… ì´ ìˆ˜ì§‘ëœ ì „ë¬¸ íŠ¸ë Œë“œ: {len(unique_trends)}ê°œ")
    for trend in unique_trends:
        print(f"   â€¢ {trend}")
    
    return {
        "trends": unique_trends,
        "search_info": search_info
    }

def extract_specialized_topics(text, current_year):
    """ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ"""
    topics = []
    
    # ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” íŒ¨í„´
    specialized_patterns = [
        # ê²½ì œ/ê¸ˆìœµ íŒ¨í„´
        r'(\w+\s*ETF|ETF\s*\w+)',                              # "êµ­ì±„ ETF", "ë¦¬ì¸  ETF"
        r'(\w+\s+íˆ¬ì|íˆ¬ì\s+\w+)',                            # "ESG íˆ¬ì", "íˆ¬ì ì „ëµ"
        r'(\w+\s+ì •ì±…|ì •ì±…\s+\w+)',                            # "ê¸ˆë¦¬ ì •ì±…", "ì •ì±… ë³€í™”"
        r'(\w+\s+ì‹œì¥|ì‹œì¥\s+\w+)',                            # "ë¯¸êµ­ ì‹œì¥", "ì‹œì¥ ì „ë§"
        
        # ê¸°ìˆ /ê³¼í•™ íŒ¨í„´  
        r'(\w+AI|\w+\s+AI|AI\s+\w+)',                          # "ìƒì„±AI", "AI ê¸°ìˆ "
        r'(\w+\s+ê¸°ìˆ |ê¸°ìˆ \s+\w+)',                            # "ì–‘ì ê¸°ìˆ ", "ê¸°ìˆ  í˜ì‹ "
        r'(\w+\s+ì»´í“¨íŒ…|ì»´í“¨íŒ…\s+\w+)',                       # "í´ë¼ìš°ë“œ ì»´í“¨íŒ…", "ì—£ì§€ ì»´í“¨íŒ…"
        
        # í™˜ê²½/ì§€ì†ê°€ëŠ¥ì„± íŒ¨í„´
        r'(\w+\s+ì—ë„ˆì§€|ì—ë„ˆì§€\s+\w+)',                        # "ì¬ìƒ ì—ë„ˆì§€", "ìˆ˜ì†Œ ì—ë„ˆì§€"
        r'(\w+ì¤‘ë¦½|\w+\s+ì¤‘ë¦½|ì¤‘ë¦½\s+\w+)',                    # "íƒ„ì†Œì¤‘ë¦½", "ì¤‘ë¦½ ë¡œë“œë§µ"
        r'(ESG\s+\w+|\w+\s+ESG)',                              # "ESG íˆ¬ì", "ESG ê³µì‹œ"
        r'(\w+í…Œí¬|\w+\s+í…Œí¬|í…Œí¬\s+\w+)',                    # "ê¸°í›„í…Œí¬", "í•€í…Œí¬"
        
        # ë¶„ì„/ì „ëµ íŒ¨í„´
        r'(\w+\s+ë¶„ì„|ë¶„ì„\s+\w+)',                            # "ì‹œì¥ ë¶„ì„", "ë¶„ì„ ì „ëµ"
        r'(\w+\s+ì „ëµ|ì „ëµ\s+\w+)',                            # "íˆ¬ì ì „ëµ", "ì „ëµ ìˆ˜ë¦½"
        r'(\w+\s+í™œìš©|í™œìš©\s+\w+)',                            # "CMA í™œìš©", "í™œìš©ë²•"
    ]
    
    for pattern in specialized_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, str) and 2 <= len(match) <= 25:
                # ì „ë¬¸ìš©ì–´ í•„í„°ë§
                if not any(skip in match for skip in ['ìˆìœ¼ë©°', 'ë˜ì–´', 'ì¤‘ì‹¬ìœ¼ë¡œ', 'í†µí•´']):
                    clean_match = match.strip()
                    if clean_match:
                        topics.append(clean_match)
    
    # ë³µí•© ì „ë¬¸ìš©ì–´ ì¶”ì¶œ
    compound_patterns = [
        r'(\w+\s+\w+\s+(?:ë¡œë“œë§µ|ì „ëµ|ì •ì±…|ê¸°ìˆ |ì‹œì¥))',        # "íƒ„ì†Œì¤‘ë¦½ ë¡œë“œë§µ"
        r'(\w+\s+\w+\s+(?:íˆ¬ì|ë¶„ì„|í™œìš©|ì „ë§))',              # "í•´ì™¸ì£¼ì‹ íˆ¬ì"
        r'(ë””ì§€í„¸\s+\w+|\w+\s+ë””ì§€í„¸)',                        # "ë””ì§€í„¸ ì „í™˜"
    ]
    
    for pattern in compound_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if 4 <= len(match) <= 30:
                topics.append(match.strip())
    
    # í›„ì²˜ë¦¬ - ì „ë¬¸ì„± í•„í„°ë§
    filtered_topics = []
    for topic in topics:
        topic = topic.strip()
        if (
            len(topic) >= 3 and 
            not topic.startswith(('ì´', 'ê·¸', 'í•˜', 'ìˆ', 'ë˜', 'ê²ƒ')) and
            not topic.endswith(('ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—ì„œ', 'ìœ¼ë¡œ')) and
            topic not in ['ìƒë°˜ê¸°', 'ì‹œì ì—ì„œ', 'í•µì‹¬', 'íŠ¹íˆ', 'ì¤‘ì‹¬ìœ¼ë¡œ', 'í†µí•´']
        ):
            filtered_topics.append(topic)
    
    return list(set(filtered_topics))[:7]  # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 7ê°œ

def test_specialized_ai_topic_generation(trend_data, date_info):
    """ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” AI ì£¼ì œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ¤– 4. ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” AI ì£¼ì œ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("-" * 35)
    
    if not openai.api_key:
        print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ì „ë¬¸ ë¶„ì•¼ ëŒ€ì²´ ì£¼ì œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
        return get_specialized_fallback_topics(date_info)
    
    try:
        current_date = date_info["date_str"]
        season = date_info["season"]
        trends_text = ", ".join(trend_data["trends"][:8])  # ë” ë§ì€ ì „ë¬¸ íŠ¸ë Œë“œ í¬í•¨
        
        # ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” í”„ë¡¬í”„íŠ¸
        specialized_prompt = f"""
í˜„ì¬ ì‹œì : {current_date} ({season})

ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ìˆ˜ì§‘í•œ ì „ë¬¸ ë¶„ì•¼ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ:
{trends_text}

ìœ„ íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•˜ì—¬ ë‹¤ìŒ ì „ë¬¸ ë¶„ì•¼ì— íŠ¹í™”ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì£¼ì œ 6ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”:

ì „ë¬¸ ë¶„ì•¼: ê¸°í›„ë³€í™”, ì¬í…Œí¬, ì •ë³´ê¸°ìˆ , ë¯¸êµ­ì£¼ì‹, ê³¼í•™ê¸°ìˆ , ì§€ì†ê°€ëŠ¥ì„±, í™˜ê²½ì •ì±…, êµ­ë‚´ì£¼ì‹, í•œêµ­/ê¸€ë¡œë²Œ ê²½ì œ

ì¡°ê±´:
- ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ í™œìš©
- ì „ë¬¸ì ì´ê³  ì‹¬ì¸µì ì¸ ë¶„ì„ ì£¼ì œ
- íˆ¬ìì/ì „ë¬¸ê°€ë“¤ì´ ê´€ì‹¬ ìˆì„ ë‚´ìš©
- "ë¶„ì„", "ì „ë§", "ì „ëµ", "ë™í–¥" ë“±ì˜ ì „ë¬¸ ìš©ì–´ í¬í•¨
- {date_info['year']}ë…„ {date_info['month']}ì›” ì‹œì ì„± ë°˜ì˜

ì¶œë ¥ í˜•ì‹:
1. ì£¼ì œëª…
2. ì£¼ì œëª…  
3. ì£¼ì œëª…
4. ì£¼ì œëª…
5. ì£¼ì œëª…
6. ì£¼ì œëª…
"""
        
        print("ğŸ”„ ì „ë¬¸ ë¶„ì•¼ AIì—ê²Œ ì£¼ì œ ìƒì„± ìš”ì²­ ì¤‘...")
        print(f"   ğŸ“¤ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(specialized_prompt)} ê¸€ì")
        print(f"   ğŸ”‘ í¬í•¨ëœ ì „ë¬¸ íŠ¸ë Œë“œ: {trends_text}")
        
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": specialized_prompt}],
            max_tokens=500,
            temperature=0.6  # ì „ë¬¸ì„±ì„ ìœ„í•´ ì˜¨ë„ ë‚®ì¶¤
        )
        
        ai_response = response.choices[0].message.content.strip()
        print(f"   ğŸ“¥ AI ì‘ë‹µ ê¸¸ì´: {len(ai_response)} ê¸€ì")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì›ë³¸ ì‘ë‹µ ì¶œë ¥
        print(f"   ğŸ“‹ AI ì›ë³¸ ì‘ë‹µ:")
        print(f"      \"{ai_response[:300]}{'...' if len(ai_response) > 300 else ''}\"")
        
        # ì „ë¬¸ ì£¼ì œ ì¶”ì¶œ
        ai_topics = parse_specialized_ai_response(ai_response)
        
        print(f"âœ… ì „ë¬¸ ë¶„ì•¼ AI ìƒì„± ì£¼ì œ ({len(ai_topics)}ê°œ):")
        for i, topic in enumerate(ai_topics, 1):
            print(f"   {i}. {topic}")
        
        return ai_topics
        
    except Exception as e:
        print(f"âŒ ì „ë¬¸ ë¶„ì•¼ AI ì£¼ì œ ìƒì„± ì‹¤íŒ¨: {e}")
        return get_specialized_fallback_topics(date_info)

def parse_specialized_ai_response(ai_response):
    """ì „ë¬¸ ë¶„ì•¼ AI ì‘ë‹µ íŒŒì‹±"""
    topics = []
    lines = ai_response.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # ë²ˆí˜¸ê°€ ìˆëŠ” ê²½ìš° (1. ì£¼ì œëª…)
        number_match = re.match(r'^\d+\.\s*(.+)', line)
        if number_match:
            topic = number_match.group(1).strip()
            if len(topic) > 8 and topic not in topics:  # ì „ë¬¸ ì£¼ì œëŠ” ë” ê¸¸ì–´ì•¼ í•¨
                topics.append(topic)
            continue
        
        # ê¸°í˜¸ê°€ ìˆëŠ” ê²½ìš° (- ì£¼ì œëª…, â€¢ ì£¼ì œëª…)
        symbol_match = re.match(r'^[â€¢\-\*]\s*(.+)', line)
        if symbol_match:
            topic = symbol_match.group(1).strip()
            if len(topic) > 8 and topic not in topics:
                topics.append(topic)
            continue
        
        # ì „ë¬¸ì ì¸ í…ìŠ¤íŠ¸ íŒ¨í„´
        if (
            len(line) > 10 and len(line) < 150 and
            any(keyword in line for keyword in ['ë¶„ì„', 'ì „ë§', 'ì „ëµ', 'ë™í–¥', 'ì •ì±…', 'íˆ¬ì', 'ê¸°ìˆ ', 'ì‹œì¥']) and
            not line.startswith(('í˜„ì¬', 'ìœ„', 'ì¡°ê±´', 'ì¶œë ¥', 'ì „ë¬¸'))
        ):
            clean_topic = line.strip('"').strip("'")
            if clean_topic and clean_topic not in topics:
                topics.append(clean_topic)
    
    return topics[:6]  # ìµœëŒ€ 6ê°œ

def get_specialized_fallback_topics(date_info):
    """ì „ë¬¸ ë¶„ì•¼ ëŒ€ì²´ ì£¼ì œ ìƒì„±"""
    current_year = date_info["year"]
    current_month = date_info["month"]
    
    # ì „ë¬¸ ë¶„ì•¼ë³„ ê³„ì ˆ ê³ ë ¤ ì£¼ì œ
    if current_month in [12, 1, 2]:  # ê²¨ìš¸/ì—°ë§ì—°ì´ˆ
        topics = [
            f"{current_year}ë…„ ì—°ë§ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ë°¸ëŸ°ì‹± ì „ëµ",
            f"{current_year+1}ë…„ ë¯¸êµ­ì£¼ì‹ íˆ¬ì ì „ë§ ë¶„ì„",
            "ê²¨ìš¸ì²  ì—ë„ˆì§€ íš¨ìœ¨ì„± ì •ì±… ë™í–¥"
        ]
    elif current_month in [3, 4, 5]:  # ë´„/ì‹ ë…„ë„ ì‹œì‘
        topics = [
            f"{current_year}ë…„ ìƒë°˜ê¸° ê²½ì œì •ì±… ë¶„ì„",
            "ë´„ì²  ì¬ìƒì—ë„ˆì§€ íˆ¬ì ê¸°íšŒ ë°œêµ´",
            f"{current_year}ë…„ AI ê¸°ìˆ ì£¼ ì„±ì¥ ì „ë§"
        ]
    elif current_month in [6, 7, 8]:  # ì—¬ë¦„/ì¤‘ê°„ ê²°ì‚°
        topics = [
            f"{current_year}ë…„ ìƒë°˜ê¸° ì£¼ì‹ì‹œì¥ ê²°ì‚° ë¶„ì„",
            "ì—¬ë¦„ì²  ì „ë ¥ ìˆ˜ê¸‰ê³¼ ì—ë„ˆì§€ ì •ì±… ë™í–¥",
            f"{current_year}ë…„ í•˜ë°˜ê¸° IT ê¸°ìˆ  íŠ¸ë Œë“œ ì „ë§"
        ]
    else:  # ê°€ì„/3ë¶„ê¸° ê²°ì‚°
        topics = [
            f"{current_year}ë…„ 3ë¶„ê¸° ê²½ì œ ì§€í‘œ ë¶„ì„",
            "ê°€ì„ì²  ESG íˆ¬ì ë™í–¥ê³¼ ì „ë§",
            f"{current_year}ë…„ ì—°ë§ ì¬í…Œí¬ ì „ëµ ìˆ˜ë¦½"
        ]
    
    print(f"âœ… ì „ë¬¸ ë¶„ì•¼ ëŒ€ì²´ ì£¼ì œ ìƒì„± ({len(topics)}ê°œ):")
    for i, topic in enumerate(topics, 1):
        print(f"   {i}. {topic}")
    
    return topics

def test_specialized_final_selection(date_info, trend_data, ai_topics):
    """ì „ë¬¸ ë¶„ì•¼ ìµœì¢… ì£¼ì œ ì„ íƒ ì‹œìŠ¤í…œ"""
    print(f"\nğŸ¯ 5. ì „ë¬¸ ë¶„ì•¼ ìµœì¢… ì£¼ì œ ì„ íƒ ì‹œìŠ¤í…œ")
    print("-" * 40)
    
    all_topics = []
    
    # ì „ë¬¸ AI ì¶”ì²œ ì£¼ì œ
    if ai_topics:
        print(f"ğŸ¤– ì‹¤ì‹œê°„ ì „ë¬¸ AI ì¶”ì²œ ({len(ai_topics)}ê°œ):")
        for topic in ai_topics:
            all_topics.append(f"[ì „ë¬¸ AI] {topic}")
    
    # ì „ë¬¸ ì›¹ íŠ¸ë Œë“œ
    direct_trends = trend_data["trends"][:4]
    if direct_trends:
        print(f"\nğŸ”¥ ì‹¤ì‹œê°„ ì „ë¬¸ íŠ¸ë Œë“œ ({len(direct_trends)}ê°œ):")
        for topic in direct_trends:
            # ì „ë¬¸ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë¶„ì„/ì „ë§ ì¶”ê°€
            if any(word in topic for word in ['íˆ¬ì', 'ì •ì±…', 'ê¸°ìˆ ', 'ì‹œì¥']):
                all_topics.append(f"[ì›¹ íŠ¸ë Œë“œ] {topic} ì‹¬ì¸µ ë¶„ì„")
            else:
                all_topics.append(f"[ì›¹ íŠ¸ë Œë“œ] {topic} ë™í–¥ ë¶„ì„")
    
    # ì „ë¬¸ ê¸°ë³¸ ì£¼ì œ
    fallback_topics = get_specialized_fallback_topics(date_info)[:2]
    print(f"\nğŸ“š ì „ë¬¸ ê¸°ë³¸ ì•ˆì • ì£¼ì œ ({len(fallback_topics)}ê°œ):")
    for topic in fallback_topics:
        all_topics.append(f"[ê¸°ë³¸] {topic}")
    
    # ìµœì¢… ì „ë¬¸ ì£¼ì œ ëª©ë¡ ì¶œë ¥
    print(f"\n{'='*70}")
    print(f"ğŸ¯ ì „ë¬¸ ë¸”ë¡œê·¸ ìµœì¢… ì¶”ì²œ ì£¼ì œ ({date_info['date_str']} ê¸°ì¤€):")
    print(f"{'='*70}")
    
    for i, topic in enumerate(all_topics, 1):
        print(f"{i:2d}. {topic}")
    
    print(f"{len(all_topics)+1:2d}. ì§ì ‘ ì…ë ¥")
    print(f"{len(all_topics)+2:2d}. ì‹¤ì‹œê°„ ì „ë¬¸ íŠ¸ë Œë“œ ìƒˆë¡œê³ ì¹¨")
    
    return all_topics

def main():
    print("ğŸ§ª ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print("ğŸ“Š ì „ë¬¸ ë¶„ì•¼: ê¸°í›„ë³€í™”, ì¬í…Œí¬, ì •ë³´ê¸°ìˆ , ë¯¸êµ­ì£¼ì‹, ê³¼í•™ê¸°ìˆ ,")
    print("            ì§€ì†ê°€ëŠ¥ì„±, í™˜ê²½ì •ì±…, êµ­ë‚´ì£¼ì‹, í•œêµ­/ê¸€ë¡œë²Œ ê²½ì œ")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        # 1. ë™ì  ë‚ ì§œ ê°ì§€
        date_info = test_dynamic_date_detection()
        
        # 2. ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        specialized_queries = test_specialized_queries(date_info)
        
        # 3. ì „ë¬¸ ë¶„ì•¼ ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        trend_data = test_specialized_web_search_mock(specialized_queries)
        
        # 4. ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” AI ì£¼ì œ ìƒì„±
        ai_topics = test_specialized_ai_topic_generation(trend_data, date_info)
        
        # 5. ì „ë¬¸ ë¶„ì•¼ ìµœì¢… ì£¼ì œ ì„ íƒ ì‹œìŠ¤í…œ
        final_topics = test_specialized_final_selection(date_info, trend_data, ai_topics)
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        end_time = time.time()
        print(f"\nğŸ‰ ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        print(f"\nğŸ“Š ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” ê²°ê³¼:")
        print(f"   â€¢ ê°ì§€ëœ í˜„ì¬ ì‹œì : {date_info['date_str']}")
        print(f"   â€¢ ìƒì„±ëœ ì „ë¬¸ ì¿¼ë¦¬: {len(specialized_queries)}ê°œ")
        print(f"   â€¢ ìˆ˜ì§‘ëœ ì „ë¬¸ íŠ¸ë Œë“œ: {len(trend_data['trends'])}ê°œ")
        print(f"   â€¢ ì „ë¬¸ AI ìƒì„± ì£¼ì œ: {len(ai_topics)}ê°œ")
        print(f"   â€¢ ìµœì¢… ì „ë¬¸ ì¶”ì²œ ì£¼ì œ: {len(final_topics)}ê°œ")
        
        # ì „ë¬¸ì„± ê°œì„ ì‚¬í•­ ì²´í¬
        improvements = []
        if len(ai_topics) > 0:
            improvements.append("âœ… ì „ë¬¸ ë¶„ì•¼ AI ì£¼ì œ ìƒì„± ì„±ê³µ")
        if len(trend_data['trends']) > 3:
            improvements.append("âœ… ì „ë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ")
        if any('ë¶„ì„' in topic or 'ì „ë§' in topic or 'ì „ëµ' in topic for topic in ai_topics):
            improvements.append("âœ… ì „ë¬¸ì„± ë†’ì€ ì£¼ì œ ìƒì„± í™•ì¸")
        
        if improvements:
            print(f"\nğŸ”§ ì „ë¬¸ì„± ê°œì„ ì‚¬í•­:")
            for improvement in improvements:
                print(f"   {improvement}")
        
        print(f"\nâœ… ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ì‹œìŠ¤í…œ ì™„ì„±!")
        print(f"ğŸš€ ì „ë¬¸ ë¸”ë¡œê·¸ìš© ë©”ì¸ ì½”ë“œ ì ìš© ì¤€ë¹„ ì™„ë£Œ!")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ì „ë¬¸ ë¶„ì•¼ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nğŸ”§ ë‹¤ìŒ ë‹¨ê³„: ì „ë¬¸ ë¶„ì•¼ íŠ¹í™” ì½”ë“œë¥¼ ë©”ì¸ ì½”ë“œì— ì ìš©")
    else:
        print(f"\nâš ï¸ ë¬¸ì œ í•´ê²° í›„ ë‹¤ì‹œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”.") 