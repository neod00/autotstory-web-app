#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URL ì½˜í…ì¸  ì¶”ì¶œ ëª¨ë“ˆ
================

ë‰´ìŠ¤ ê¸°ì‚¬, ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸, ìœ íŠœë¸Œ ì˜ìƒ ë“±ì˜ URLì—ì„œ 
ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¸”ë¡œê·¸ ê¸€ ìƒì„±ì— í™œìš©í•˜ëŠ” ëª¨ë“ˆ
"""

import re
import requests
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup
import newspaper
from newspaper import Article
import feedparser
import time
import openai
import os
from dotenv import load_dotenv

# YouTube ìë§‰ ì¶”ì¶œì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì  import)
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    from youtube_transcript_api.formatters import TextFormatter
    YOUTUBE_TRANSCRIPT_AVAILABLE = True
except ImportError:
    YOUTUBE_TRANSCRIPT_AVAILABLE = False
    print("âš ï¸ youtube-transcript-apiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì • (ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë™ì¼í•œ ë°©ì‹)
openai.api_key = os.getenv("OPENAI_API_KEY")

class URLContentExtractor:
    """URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def extract_content_from_url(self, url):
        """
        URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
        
        Args:
            url (str): ì¶”ì¶œí•  URL
            
        Returns:
            dict: ì¶”ì¶œëœ ì½˜í…ì¸  ì •ë³´
        """
        try:
            # URL ìœ í˜• íŒë‹¨
            url_type = self._detect_url_type(url)
            
            print(f"ğŸ” URL íƒ€ì… ê°ì§€: {url_type}")
            print(f"ğŸ“ ì²˜ë¦¬í•  URL: {url}")
            
            if url_type == 'youtube':
                return self._extract_youtube_content(url)
            elif url_type == 'news':
                return self._extract_news_content(url)
            elif url_type == 'blog':
                return self._extract_blog_content(url)
            else:
                return self._extract_generic_content(url)
                
        except Exception as e:
            print(f"âŒ URL ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'title': '',
                'content': '',
                'summary': '',
                'url': url
            }
    
    def _detect_url_type(self, url):
        """URL íƒ€ì…ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜"""
        domain = urlparse(url).netloc.lower()
        
        # ìœ íŠœë¸Œ ê°ì§€
        if 'youtube.com' in domain or 'youtu.be' in domain:
            return 'youtube'
        
        # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ê°ì§€
        news_domains = [
            'naver.com', 'daum.net', 'chosun.com', 'joongang.co.kr',
            'donga.com', 'hani.co.kr', 'khan.co.kr', 'ytn.co.kr',
            'sbs.co.kr', 'mbc.co.kr', 'kbs.co.kr', 'newsis.com',
            'yna.co.kr', 'mk.co.kr', 'mt.co.kr', 'edaily.co.kr',
            'bbc.com', 'cnn.com', 'reuters.com', 'ap.org'
        ]
        
        for news_domain in news_domains:
            if news_domain in domain:
                return 'news'
        
        # ë¸”ë¡œê·¸ í”Œë«í¼ ê°ì§€
        blog_domains = [
            'tistory.com', 'blog.naver.com', 'brunch.co.kr',
            'medium.com', 'wordpress.com', 'blogger.com'
        ]
        
        for blog_domain in blog_domains:
            if blog_domain in domain:
                return 'blog'
        
        return 'generic'
    
    def _extract_youtube_content(self, url):
        """ìœ íŠœë¸Œ ì˜ìƒ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            # ìœ íŠœë¸Œ ì˜ìƒ ID ì¶”ì¶œ
            video_id = self._extract_video_id(url)
            if not video_id:
                raise Exception("ìœ íŠœë¸Œ ì˜ìƒ IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì˜ìƒ ì •ë³´ ì¶”ì¶œ (ì œëª©, ì„¤ëª…)
            video_info = self._get_youtube_video_info(video_id)
            
            # ìë§‰ ì¶”ì¶œ ì‹œë„ (YouTube Transcript API ì—†ì´ ì›¹ ìŠ¤í¬ë˜í•‘ ë°©ì‹)
            transcript = self._get_youtube_transcript_web(video_id)
            
            # AIë¥¼ ì´ìš©í•œ ì½˜í…ì¸  ìš”ì•½
            summary = self._summarize_youtube_content(video_info, transcript)
            
            return {
                'success': True,
                'type': 'youtube',
                'title': video_info.get('title', ''),
                'content': transcript,
                'summary': summary,
                'description': video_info.get('description', ''),
                'url': url,
                'video_id': video_id
            }
            
        except Exception as e:
            print(f"âŒ ìœ íŠœë¸Œ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'title': '',
                'content': '',
                'summary': '',
                'url': url
            }
    
    def _extract_news_content(self, url):
        """ë‰´ìŠ¤ ê¸°ì‚¬ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            # newspaper3k ì‚¬ìš©
            article = Article(url)
            article.download()
            article.parse()
            
            # í•œêµ­ì–´ ê¸°ì‚¬ì˜ ê²½ìš° ì¶”ê°€ ì²˜ë¦¬
            if not article.text:
                # BeautifulSoupì„ ì‚¬ìš©í•œ ë³´ì¡° ì¶”ì¶œ
                response = self.session.get(url, timeout=10)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ (ì¼ë°˜ì ì¸ íƒœê·¸ë“¤)
                content_selectors = [
                    'article', '.article-body', '.news-content', 
                    '.content', '.post-content', '#newsContent',
                    '.article-content', '.news-article'
                ]
                
                content = ""
                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        content = elements[0].get_text(strip=True)
                        break
                
                if not content:
                    # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ê¸´ p íƒœê·¸ë“¤ ì¶”ì¶œ
                    paragraphs = soup.find_all('p')
                    long_paragraphs = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 100]
                    content = '\n\n'.join(long_paragraphs[:10])  # ìƒìœ„ 10ê°œ ë¬¸ë‹¨
                
                article.text = content
            
            # AIë¥¼ ì´ìš©í•œ ë‰´ìŠ¤ ìš”ì•½
            summary = self._summarize_news_content(article.title, article.text)
            
            return {
                'success': True,
                'type': 'news',
                'title': article.title or '',
                'content': article.text or '',
                'summary': summary,
                'authors': article.authors,
                'publish_date': article.publish_date,
                'url': url
            }
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self._extract_generic_content(url)
    
    def _extract_blog_content(self, url):
        """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            # newspaper3k ì‚¬ìš©
            article = Article(url)
            article.download()
            article.parse()
            
            # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
            meta_description = ""
            meta_tag = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
            if meta_tag:
                meta_description = meta_tag.get('content', '')
            
            # AIë¥¼ ì´ìš©í•œ ë¸”ë¡œê·¸ ìš”ì•½
            summary = self._summarize_blog_content(article.title, article.text, meta_description)
            
            return {
                'success': True,
                'type': 'blog',
                'title': article.title or '',
                'content': article.text or '',
                'summary': summary,
                'meta_description': meta_description,
                'authors': article.authors,
                'publish_date': article.publish_date,
                'url': url
            }
            
        except Exception as e:
            print(f"âŒ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self._extract_generic_content(url)
    
    def _extract_generic_content(self, url):
        """ì¼ë°˜ ì›¹í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = ""
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            # ë‹¤ì–‘í•œ ì½˜í…ì¸  ì„ íƒì ì‹œë„
            content_selectors = [
                'main', 'article', '.content', '.post-content',
                '.entry-content', '.post-body', '.article-body',
                '.page-content', '#content', '.main-content'
            ]
            
            content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0].get_text(strip=True)
                    break
            
            # ì„ íƒìë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° p íƒœê·¸ë“¤ ì¶”ì¶œ
            if not content:
                paragraphs = soup.find_all('p')
                long_paragraphs = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50]
                content = '\n\n'.join(long_paragraphs[:15])
            
            # AIë¥¼ ì´ìš©í•œ ì½˜í…ì¸  ìš”ì•½
            summary = self._summarize_generic_content(title, content)
            
            return {
                'success': True,
                'type': 'generic',
                'title': title,
                'content': content,
                'summary': summary,
                'url': url
            }
            
        except Exception as e:
            print(f"âŒ ì¼ë°˜ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'title': '',
                'content': '',
                'summary': '',
                'url': url
            }
    
    def _extract_video_id(self, url):
        """ìœ íŠœë¸Œ URLì—ì„œ ì˜ìƒ ID ì¶”ì¶œ"""
        patterns = [
            r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
            r'(?:embed\/)([0-9A-Za-z_-]{11})',
            r'(?:watch\?v=)([0-9A-Za-z_-]{11})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return None
    
    def _get_youtube_video_info(self, video_id):
        """ìœ íŠœë¸Œ ì˜ìƒ ì •ë³´ ì¶”ì¶œ (ì œëª©, ì„¤ëª…)"""
        try:
            # ìœ íŠœë¸Œ í˜ì´ì§€ ì§ì ‘ íŒŒì‹±
            url = f"https://www.youtube.com/watch?v={video_id}"
            response = self.session.get(url, timeout=10)
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = ""
            title_tag = soup.find('meta', attrs={'property': 'og:title'})
            if title_tag:
                title = title_tag.get('content', '')
            
            # ì„¤ëª… ì¶”ì¶œ
            description = ""
            desc_tag = soup.find('meta', attrs={'property': 'og:description'})
            if desc_tag:
                description = desc_tag.get('content', '')
            
            return {
                'title': title,
                'description': description
            }
            
        except Exception as e:
            print(f"âŒ ìœ íŠœë¸Œ ì˜ìƒ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'title': '', 'description': ''}
    
    def _get_youtube_transcript_web(self, video_id):
        """ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ (YouTube Transcript API ì‚¬ìš©)"""
        try:
            if not YOUTUBE_TRANSCRIPT_AVAILABLE:
                return "YouTube Transcript APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ìë§‰ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ìƒ ì„¤ëª…ì„ ì°¸ê³ í•˜ì„¸ìš”."
            
            # í•œêµ­ì–´ ìë§‰ ìš°ì„  ì‹œë„, ì—†ìœ¼ë©´ ì˜ì–´, ê·¸ ë‹¤ìŒ ìë™ ìƒì„± ìë§‰
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # ìˆ˜ë™ í•œêµ­ì–´ ìë§‰ ì°¾ê¸°
            try:
                transcript = transcript_list.find_manually_created_transcript(['ko'])
                formatter = TextFormatter()
                transcript_data = transcript.fetch()
                transcript_text = formatter.format_transcript(transcript_data)
                print(f"   âœ… í•œêµ­ì–´ ìˆ˜ë™ ìë§‰ ë°œê²¬ (ê¸¸ì´: {len(transcript_text)} ê¸€ì)")
                return transcript_text
            except:
                pass
            
            # ìˆ˜ë™ ì˜ì–´ ìë§‰ ì°¾ê¸°
            try:
                transcript = transcript_list.find_manually_created_transcript(['en'])
                formatter = TextFormatter()
                transcript_data = transcript.fetch()
                transcript_text = formatter.format_transcript(transcript_data)
                print(f"   âœ… ì˜ì–´ ìˆ˜ë™ ìë§‰ ë°œê²¬ (ê¸¸ì´: {len(transcript_text)} ê¸€ì)")
                return transcript_text
            except:
                pass
            
            # ìë™ ìƒì„± ìë§‰ ì°¾ê¸° (í•œêµ­ì–´ ìš°ì„ )
            try:
                transcript = transcript_list.find_generated_transcript(['ko'])
                formatter = TextFormatter()
                transcript_data = transcript.fetch()
                transcript_text = formatter.format_transcript(transcript_data)
                print(f"   âœ… í•œêµ­ì–´ ìë™ ìë§‰ ë°œê²¬ (ê¸¸ì´: {len(transcript_text)} ê¸€ì)")
                return transcript_text
            except:
                pass
            
            # ìë™ ìƒì„± ìë§‰ ì°¾ê¸° (ì˜ì–´)
            try:
                transcript = transcript_list.find_generated_transcript(['en'])
                formatter = TextFormatter()
                transcript_data = transcript.fetch()
                transcript_text = formatter.format_transcript(transcript_data)
                print(f"   âœ… ì˜ì–´ ìë™ ìë§‰ ë°œê²¬ (ê¸¸ì´: {len(transcript_text)} ê¸€ì)")
                return transcript_text
            except:
                pass
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ìë§‰ ì‚¬ìš©
            try:
                for transcript in transcript_list:
                    try:
                        formatter = TextFormatter()
                        transcript_data = transcript.fetch()
                        transcript_text = formatter.format_transcript(transcript_data)
                        language = transcript.language_code
                        print(f"   âœ… {language} ìë§‰ ë°œê²¬ (ê¸¸ì´: {len(transcript_text)} ê¸€ì)")
                        return transcript_text
                    except:
                        continue
            except:
                pass
            
            return "ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ìƒ ì„¤ëª…ì„ ì°¸ê³ í•˜ì„¸ìš”."
            
        except Exception as e:
            print(f"âŒ ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "ìë§‰ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜ìƒ ì„¤ëª…ì„ ì°¸ê³ í•˜ì„¸ìš”."
    
    def _summarize_youtube_content(self, video_info, transcript):
        """ìœ íŠœë¸Œ ì½˜í…ì¸  ìš”ì•½"""
        if not os.getenv("OPENAI_API_KEY"):
            return "AI ìš”ì•½ì„ ìœ„í•œ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            content = f"ì œëª©: {video_info.get('title', '')}\nì„¤ëª…: {video_info.get('description', '')}\nìë§‰: {transcript}"
            
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ìœ íŠœë¸Œ ì˜ìƒ ë‚´ìš©ì„ ë¸”ë¡œê·¸ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": f"ë‹¤ìŒ ìœ íŠœë¸Œ ì˜ìƒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ìœ„í•œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”:\n\n{content}"}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ ìœ íŠœë¸Œ ì½˜í…ì¸  ìš”ì•½ ì‹¤íŒ¨: {e}")
            return video_info.get('description', '')
    
    def _summarize_news_content(self, title, content):
        """ë‰´ìŠ¤ ì½˜í…ì¸  ìš”ì•½"""
        if not os.getenv("OPENAI_API_KEY"):
            return "AI ìš”ì•½ì„ ìœ„í•œ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¸”ë¡œê·¸ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ìœ„í•œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”:\n\nì œëª©: {title}\n\në‚´ìš©: {content[:3000]}"}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ì½˜í…ì¸  ìš”ì•½ ì‹¤íŒ¨: {e}")
            return content[:500] + "..."
    
    def _summarize_blog_content(self, title, content, meta_description):
        """ë¸”ë¡œê·¸ ì½˜í…ì¸  ìš”ì•½"""
        if not os.getenv("OPENAI_API_KEY"):
            return "AI ìš”ì•½ì„ ìœ„í•œ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì¬êµ¬ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": f"ë‹¤ìŒ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë¸”ë¡œê·¸ ê¸€ì„ ìœ„í•œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”:\n\nì œëª©: {title}\nì„¤ëª…: {meta_description}\n\në‚´ìš©: {content[:3000]}"}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìš”ì•½ ì‹¤íŒ¨: {e}")
            return content[:500] + "..."
    
    def _summarize_generic_content(self, title, content):
        """ì¼ë°˜ ì½˜í…ì¸  ìš”ì•½"""
        if not os.getenv("OPENAI_API_KEY"):
            return "AI ìš”ì•½ì„ ìœ„í•œ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ì›¹ ì½˜í…ì¸ ë¥¼ ë¸”ë¡œê·¸ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": f"ë‹¤ìŒ ì›¹ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ìœ„í•œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”:\n\nì œëª©: {title}\n\në‚´ìš©: {content[:3000]}"}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ ì¼ë°˜ ì½˜í…ì¸  ìš”ì•½ ì‹¤íŒ¨: {e}")
            return content[:500] + "..."


def generate_blog_from_url(url, custom_angle=""):
    """
    URLì—ì„œ ì¶”ì¶œí•œ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        url (str): ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  URL
        custom_angle (str): ì‚¬ìš©ìê°€ ì§€ì •í•œ ê´€ì ì´ë‚˜ ê°ë„
        
    Returns:
        dict: ìƒì„±ëœ ë¸”ë¡œê·¸ ê¸€ ì •ë³´
    """
    extractor = URLContentExtractor()
    
    # 1. URLì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
    print("ğŸ” URLì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ ì¤‘...")
    extracted_content = extractor.extract_content_from_url(url)
    
    if not extracted_content['success']:
        return {
            'success': False,
            'error': extracted_content['error'],
            'title': '',
            'content': '',
            'tags': ''
        }
    
    # 2. ì¶”ì¶œëœ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ ìƒì„±
    print("ğŸ¤– ì¶”ì¶œëœ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì¤‘...")
    
    try:
        if not os.getenv("OPENAI_API_KEY"):
            raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        source_info = f"ì›ë³¸ ì œëª©: {extracted_content['title']}\nì›ë³¸ URL: {url}\n"
        source_content = extracted_content['summary'] or extracted_content['content']
        
        angle_instruction = f"\nê´€ì /ê°ë„: {custom_angle}\n" if custom_angle else ""
        
        prompt = f"""ë‹¤ìŒ {extracted_content['type']} ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  í’ë¶€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

{source_info}

ì›ë³¸ ë‚´ìš©:
{source_content[:4000]}

{angle_instruction}

ìš”êµ¬ì‚¬í•­:
1. ì›ë³¸ ë‚´ìš©ì„ ì°¸ê³ í•˜ë˜, ì™„ì „íˆ ìƒˆë¡œìš´ ê´€ì ì—ì„œ ì¬êµ¬ì„±
2. í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ì „ë¬¸ì ì´ê³  ê¹Šì´ ìˆëŠ” ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±
3. ë¸”ë¡œê·¸ ë…ìì—ê²Œ ì‹¤ìš©ì ì´ê³  ê°€ì¹˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µ
4. êµ¬ì²´ì ì¸ ì‚¬ë¡€, ë°ì´í„°, íŒ, ë…¸í•˜ìš°ë¥¼ í¬í•¨
5. ë…ìì˜ ê¶ê¸ˆì¦ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì ì¸ ì¡°ì–¸ í¬í•¨
6. ê´€ë ¨ ì—…ê³„ ë™í–¥, ìµœì‹  ì •ë³´, ì „ë¬¸ê°€ ê´€ì ì„ ë°˜ì˜
7. ë…ìê°€ ì‹¤ì œë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì•¡ì…˜ í”Œëœ ì œì‹œ
8. ì ì ˆí•œ ì œëª©ê³¼ êµ¬ì¡°í™”ëœ ë‚´ìš© (ë„ì…ë¶€, ë³¸ë¡ , ê²°ë¡ )
9. ê´€ë ¨ í‚¤ì›Œë“œì™€ íƒœê·¸ í¬í•¨

ì‘ì„± ê°€ì´ë“œ:
- ë„ì…ë¶€: ì£¼ì œì˜ ì¤‘ìš”ì„±ê³¼ ë…ìì—ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥ ì„¤ëª…
- ë³¸ë¡ : êµ¬ì²´ì ì¸ ë°©ë²•ë¡ , ì‚¬ë¡€, íŒ, ì£¼ì˜ì‚¬í•­ ë“±ì„ ìƒì„¸íˆ ì„¤ëª…
- ê²°ë¡ : í•µì‹¬ ìš”ì•½ê³¼ ì‹¤ì²œ ë°©ì•ˆ ì œì‹œ
- ê° ì„¹ì…˜ì€ ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ëª…í™•í•˜ê²Œ êµ¬ë¶„
- ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì‹¤ìš©ì ì¸ ì˜ˆì‹œ í¬í•¨

ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
TITLE: [ë§¤ë ¥ì ì´ê³  êµ¬ì²´ì ì¸ ë¸”ë¡œê·¸ ì œëª©]
CONTENT: [ì „ë¬¸ì ì´ê³  í’ë¶€í•œ ë¸”ë¡œê·¸ ë³¸ë¬¸ - ìµœì†Œ 2000ì ì´ìƒ]
TAGS: [ê´€ë ¨ íƒœê·¸ë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„]
"""

        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "URL ê¸°ë°˜ ì½˜í…ì¸ ë¥¼ ì¬êµ¬ì„±í•˜ëŠ” ë¸”ë¡œê·¸ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì „ë¬¸ì ì´ê³  ê¹Šì´ ìˆëŠ” ë‚´ìš©ì„ ì‘ì„±í•˜ë©°, ë…ìì—ê²Œ ì‹¤ìš©ì ì¸ ê°€ì¹˜ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=4000,
            temperature=0.8
        )
        
        generated_content = response.choices[0].message.content
        
        # ì‘ë‹µ íŒŒì‹±
        title = ""
        content = ""
        tags = ""
        
        lines = generated_content.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if line.startswith('TITLE:'):
                current_section = 'title'
                title = line.replace('TITLE:', '').strip()
            elif line.startswith('CONTENT:'):
                current_section = 'content'
                content = line.replace('CONTENT:', '').strip()
            elif line.startswith('TAGS:'):
                current_section = 'tags'
                tags = line.replace('TAGS:', '').strip()
            elif current_section == 'content' and line:
                content += '\n' + line
            elif current_section == 'tags' and line:
                tags += ' ' + line
        
        return {
            'success': True,
            'title': title or extracted_content['title'],
            'content': content or extracted_content['summary'],
            'tags': tags,
            'source_url': url,
            'source_type': extracted_content['type'],
            'original_title': extracted_content['title']
        }
        
    except Exception as e:
        print(f"âŒ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            'success': False,
            'error': str(e),
            'title': extracted_content['title'],
            'content': extracted_content['summary'],
            'tags': ''
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    test_url = input("í…ŒìŠ¤íŠ¸í•  URLì„ ì…ë ¥í•˜ì„¸ìš”: ")
    custom_angle = input("íŠ¹ë³„í•œ ê´€ì ì´ë‚˜ ê°ë„ê°€ ìˆë‹¤ë©´ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­): ")
    
    result = generate_blog_from_url(test_url, custom_angle)
    
    if result['success']:
        print("\nğŸ‰ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì„±ê³µ!")
        print(f"ì œëª©: {result['title']}")
        print(f"íƒœê·¸: {result['tags']}")
        print(f"ë‚´ìš© ê¸¸ì´: {len(result['content'])} ê¸€ì")
        print(f"\në‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{result['content'][:500]}...")
    else:
        print(f"\nâŒ ì‹¤íŒ¨: {result['error']}") 