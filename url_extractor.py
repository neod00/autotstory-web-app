#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URL ì½˜í…ì¸  ì¶”ì¶œ ëª¨ë“ˆ - Streamlit ì•±ìš©
================================

ë‰´ìŠ¤ ê¸°ì‚¬, ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸, ìœ íŠœë¸Œ ì˜ìƒ ë“±ì˜ URLì—ì„œ 
ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¸”ë¡œê·¸ ê¸€ ìƒì„±ì— í™œìš©í•˜ëŠ” ëª¨ë“ˆ
"""

import re
import requests
from urllib.parse import urlparse, parse_qs
from bs4 import BeautifulSoup
import time
import openai
import os
from typing import Dict, Optional, List
import random

# streamlit importë¥¼ ì¡°ê±´ë¶€ë¡œ ì²˜ë¦¬
try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False
    # streamlitì´ ì—†ì„ ë•Œë¥¼ ìœ„í•œ ë”ë¯¸ í•¨ìˆ˜
    def st_info(msg): print(f"INFO: {msg}")
    def st_success(msg): print(f"SUCCESS: {msg}")
    def st_warning(msg): print(f"WARNING: {msg}")
    def st_error(msg): print(f"ERROR: {msg}")
    st = type('st', (), {
        'info': st_info,
        'success': st_success,
        'warning': st_warning,
        'error': st_error
    })()

# YouTube ìë§‰ ì¶”ì¶œì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì  import)
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    from youtube_transcript_api.formatters import TextFormatter
    YOUTUBE_TRANSCRIPT_AVAILABLE = True
except ImportError:
    YOUTUBE_TRANSCRIPT_AVAILABLE = False
    st.warning("âš ï¸ youtube-transcript-apiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

class URLContentExtractor:
    """URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def extract_content_from_url(self, url: str) -> Dict:
        """
        URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
        
        Args:
            url (str): ì¶”ì¶œí•  URL
            
        Returns:
            dict: ì¶”ì¶œëœ ì½˜í…ì¸  ì •ë³´
        """
        try:
            # URL ìœ í˜• íŒë‹¨
            url_type = self._detect_url_type(url)
            
            st.info(f"ğŸ” URL íƒ€ì… ê°ì§€: {url_type}")
            st.info(f"ğŸ“ ì²˜ë¦¬í•  URL: {url}")
            
            if url_type == 'youtube':
                return self._extract_youtube_content(url)
            elif url_type == 'news':
                return self._extract_news_content(url)
            elif url_type == 'blog':
                return self._extract_blog_content(url)
            else:
                return self._extract_generic_content(url)
                
        except Exception as e:
            st.error(f"âŒ URL ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'title': '',
                'content': '',
                'summary': '',
                'url': url
            }
    
    def _detect_url_type(self, url: str) -> str:
        """URL íƒ€ì…ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜"""
        domain = urlparse(url).netloc.lower()
        
        # ìœ íŠœë¸Œ ê°ì§€
        if 'youtube.com' in domain or 'youtu.be' in domain:
            return 'youtube'
        
        # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ê°ì§€
        news_domains = [
            'naver.com', 'daum.net', 'chosun.com', 'joongang.co.kr',
            'donga.com', 'hani.co.kr', 'khan.co.kr', 'ytn.co.kr',
            'sbs.co.kr', 'mbc.co.kr', 'kbs.co.kr', 'newsis.com',
            'yna.co.kr', 'mk.co.kr', 'mt.co.kr', 'edaily.co.kr',
            'bbc.com', 'cnn.com', 'reuters.com', 'ap.org'
        ]
        
        for news_domain in news_domains:
            if news_domain in domain:
                return 'news'
        
        # ë¸”ë¡œê·¸ í”Œë«í¼ ê°ì§€
        blog_domains = [
            'tistory.com', 'blog.naver.com', 'blog.daum.net',
            'medium.com', 'wordpress.com', 'blogspot.com'
        ]
        
        for blog_domain in blog_domains:
            if blog_domain in domain:
                return 'blog'
        
        return 'generic'
    
    def _extract_youtube_content(self, url: str) -> Dict:
        """ìœ íŠœë¸Œ ì˜ìƒì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            video_id = self._extract_video_id(url)
            if not video_id:
                return {'success': False, 'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ YouTube URLì…ë‹ˆë‹¤.'}
            
            # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            video_info = self._get_youtube_video_info(video_id)
            
            # ìë§‰ ì¶”ì¶œ ì‹œë„
            transcript = self._get_youtube_transcript_web(video_id)

            # ìë§‰ì´ ì •ìƒì ìœ¼ë¡œ ì¶”ì¶œëœ ê²½ìš°ì—ë§Œ ìš”ì•½ ìƒì„±
            if transcript and not (
                transcript.startswith('ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨:') or
                transcript.startswith('ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.') or
                transcript.startswith('YouTube Transcript APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„')
            ):
                summary = self._summarize_youtube_content(video_info, transcript)
                return {
                    'success': True,
                    'title': video_info.get('title', ''),
                    'content': summary,
                    'summary': summary,
                    'url': url,
                    'source_type': 'youtube',
                    'original_title': video_info.get('title', ''),
                    'video_id': video_id,
                    'duration': video_info.get('duration', ''),
                    'channel': video_info.get('channel', ''),
                    'transcript': transcript
                }
            else:
                # ìë§‰ì´ ì—†ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜
                return {
                    'success': False,
                    'error': f'ìœ íŠœë¸Œ ìë§‰ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì‚¬ìœ : {transcript}',
                    'title': video_info.get('title', ''),
                    'content': '',
                    'summary': '',
                    'url': url,
                    'source_type': 'youtube',
                    'original_title': video_info.get('title', ''),
                    'video_id': video_id,
                    'duration': video_info.get('duration', ''),
                    'channel': video_info.get('channel', ''),
                    'transcript': transcript
                }
        except Exception as e:
            return {'success': False, 'error': f'YouTube ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}'}
    
    def _extract_news_content(self, url: str) -> Dict:
        """ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = ''
            title_selectors = ['h1', '.title', '.headline', '.article-title', 'title']
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    break
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = ''
            content_selectors = [
                '.article-content', '.news-content', '.content', 
                '.article-body', '.post-content', 'article'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for elem in content_elem.find_all(['script', 'style', 'nav', 'header', 'footer']):
                        elem.decompose()
                    content = content_elem.get_text().strip()
                    break
            
            if not content:
                # fallback: ëª¨ë“  p íƒœê·¸ ìˆ˜ì§‘
                paragraphs = soup.find_all('p')
                content = '\n\n'.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 50])
            
            # ìš”ì•½ ìƒì„±
            summary = self._summarize_news_content(title, content)
            
            return {
                'success': True,
                'title': title,
                'content': content,
                'summary': summary,
                'url': url,
                'source_type': 'news',
                'original_title': title
            }
            
        except Exception as e:
            return {'success': False, 'error': f'ë‰´ìŠ¤ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}'}
    
    def _extract_blog_content(self, url: str) -> Dict:
        """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = ''
            title_selectors = ['h1', '.post-title', '.entry-title', '.title', 'title']
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    break
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = ''
            content_selectors = [
                '.post-content', '.entry-content', '.content', 
                '.article-content', '.blog-content'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for elem in content_elem.find_all(['script', 'style', 'nav', 'header', 'footer']):
                        elem.decompose()
                    content = content_elem.get_text().strip()
                    break
            
            # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
            meta_description = ''
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                meta_description = meta_desc.get('content', '')
            
            # ìš”ì•½ ìƒì„±
            summary = self._summarize_blog_content(title, content, meta_description)
            
            return {
                'success': True,
                'title': title,
                'content': content,
                'summary': summary,
                'url': url,
                'source_type': 'blog',
                'original_title': title,
                'meta_description': meta_description
            }
            
        except Exception as e:
            return {'success': False, 'error': f'ë¸”ë¡œê·¸ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}'}
    
    def _extract_generic_content(self, url: str) -> Dict:
        """ì¼ë°˜ ì›¹í˜ì´ì§€ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = ''
            title_elem = soup.find('title')
            if title_elem:
                title = title_elem.get_text().strip()
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡)
            content = ''
            text_blocks = soup.find_all(['p', 'div', 'article', 'section'])
            
            longest_block = ''
            for block in text_blocks:
                text = block.get_text().strip()
                if len(text) > len(longest_block) and len(text) > 100:
                    longest_block = text
            
            content = longest_block
            
            # ìš”ì•½ ìƒì„±
            summary = self._summarize_generic_content(title, content)
            
            return {
                'success': True,
                'title': title,
                'content': content,
                'summary': summary,
                'url': url,
                'source_type': 'generic',
                'original_title': title
            }
            
        except Exception as e:
            return {'success': False, 'error': f'ì¼ë°˜ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}'}
    
    def _extract_video_id(self, url: str) -> Optional[str]:
        """YouTube URLì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ"""
        patterns = [
            r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)',
            r'youtube\.com\/watch\?.*v=([^&\n?#]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return None
    
    def _get_youtube_video_info(self, video_id: str) -> Dict:
        """YouTube ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            url = f"https://www.youtube.com/watch?v={video_id}"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = ''
            title_elem = soup.find('title')
            if title_elem:
                title = title_elem.get_text().strip()
                title = title.replace(' - YouTube', '')
            
            # ì±„ë„ëª… ì¶”ì¶œ
            channel = ''
            channel_elem = soup.find('link', attrs={'itemprop': 'name'})
            if channel_elem:
                channel = channel_elem.get('content', '')
            
            return {
                'title': title,
                'channel': channel,
                'video_id': video_id,
                'url': url
            }
            
        except Exception as e:
            st.warning(f"YouTube ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'title': f'YouTube Video {video_id}',
                'channel': 'Unknown',
                'video_id': video_id,
                'url': f"https://www.youtube.com/watch?v={video_id}"
            }
    
    def _get_youtube_transcript_web(self, video_id: str) -> str:
        """ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ (ìµœì‹  youtube-transcript-api í˜¸í™˜)"""
        try:
            if not YOUTUBE_TRANSCRIPT_AVAILABLE:
                return "YouTube Transcript APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ìë§‰ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            transcript_data = None
            # í•œêµ­ì–´ ìš°ì„ 
            try:
                transcript_data = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko'])
            except Exception:
                try:
                    transcript_data = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
                except Exception:
                    try:
                        transcript_data = YouTubeTranscriptApi.get_transcript(video_id)
                    except Exception as e:
                        return f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
            if transcript_data:
                formatter = TextFormatter()
                return formatter.format_transcript(transcript_data)
            else:
                return "ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"
    
    def _summarize_youtube_content(self, video_info: Dict, transcript: str) -> str:
        """YouTube ì½˜í…ì¸  ìš”ì•½"""
        try:
            if not openai.api_key:
                return f"ì œëª©: {video_info.get('title', '')}\n\në‚´ìš©: {transcript[:1000]}..."
            
            prompt = f"""
ë‹¤ìŒ YouTube ì˜ìƒì˜ ë‚´ìš©ì„ ë¸”ë¡œê·¸ ê¸€ í˜•íƒœë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {video_info.get('title', '')}
ì±„ë„: {video_info.get('channel', '')}
ìë§‰ ë‚´ìš©: {transcript[:2000]}

ìš”êµ¬ì‚¬í•­:
1. 800-1200ì ì •ë„ì˜ ë¸”ë¡œê·¸ ê¸€ í˜•íƒœë¡œ ì‘ì„±
2. êµ¬ì¡°í™”ëœ í˜•íƒœ (ì„œë¡ , ë³¸ë¡ , ê²°ë¡ )
3. í•µì‹¬ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ ì „ë‹¬
4. ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"ì œëª©: {video_info.get('title', '')}\n\në‚´ìš©: {transcript[:1000]}..."
    
    def _summarize_news_content(self, title: str, content: str) -> str:
        """ë‰´ìŠ¤ ì½˜í…ì¸  ìš”ì•½"""
        try:
            if not openai.api_key:
                return content[:1000] + "..." if len(content) > 1000 else content
            
            prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¸”ë¡œê·¸ ê¸€ í˜•íƒœë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
ë‚´ìš©: {content[:2000]}

ìš”êµ¬ì‚¬í•­:
1. 800-1200ì ì •ë„ì˜ ë¸”ë¡œê·¸ ê¸€ í˜•íƒœë¡œ ì‘ì„±
2. êµ¬ì¡°í™”ëœ í˜•íƒœ (ì„œë¡ , ë³¸ë¡ , ê²°ë¡ )
3. í•µì‹¬ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ ì „ë‹¬
4. ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return content[:1000] + "..." if len(content) > 1000 else content
    
    def _summarize_blog_content(self, title: str, content: str, meta_description: str) -> str:
        """ë¸”ë¡œê·¸ ì½˜í…ì¸  ìš”ì•½"""
        try:
            if not openai.api_key:
                return content[:1000] + "..." if len(content) > 1000 else content
            
            prompt = f"""
ë‹¤ìŒ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ê°œì„ ëœ ë¸”ë¡œê·¸ ê¸€ í˜•íƒœë¡œ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
ì„¤ëª…: {meta_description}
ë‚´ìš©: {content[:2000]}

ìš”êµ¬ì‚¬í•­:
1. 800-1200ì ì •ë„ì˜ ê°œì„ ëœ ë¸”ë¡œê·¸ ê¸€ í˜•íƒœë¡œ ì‘ì„±
2. êµ¬ì¡°í™”ëœ í˜•íƒœ (ì„œë¡ , ë³¸ë¡ , ê²°ë¡ )
3. í•µì‹¬ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ ì „ë‹¬
4. ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
5. SEO ìµœì í™” ê³ ë ¤
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return content[:1000] + "..." if len(content) > 1000 else content
    
    def _summarize_generic_content(self, title: str, content: str) -> str:
        """ì¼ë°˜ ì½˜í…ì¸  ìš”ì•½"""
        try:
            if not openai.api_key:
                return content[:1000] + "..." if len(content) > 1000 else content
            
            prompt = f"""
ë‹¤ìŒ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ë¸”ë¡œê·¸ ê¸€ í˜•íƒœë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
ë‚´ìš©: {content[:2000]}

ìš”êµ¬ì‚¬í•­:
1. 800-1200ì ì •ë„ì˜ ë¸”ë¡œê·¸ ê¸€ í˜•íƒœë¡œ ì‘ì„±
2. êµ¬ì¡°í™”ëœ í˜•íƒœ (ì„œë¡ , ë³¸ë¡ , ê²°ë¡ )
3. í•µì‹¬ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ ì „ë‹¬
4. ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return content[:1000] + "..." if len(content) > 1000 else content

def generate_blog_from_url(url: str, custom_angle: str = "") -> Dict:
    """
    URL ê¸°ë°˜ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìƒì„±
    
    Args:
        url (str): ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  URL
        custom_angle (str): ì‚¬ìš©ìê°€ ì§€ì •í•œ ê´€ì ì´ë‚˜ ê°ë„
        
    Returns:
        dict: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë°ì´í„°
    """
    try:
        extractor = URLContentExtractor()
        result = extractor.extract_content_from_url(url)
        
        if not result['success']:
            return {
                'success': False,
                'error': result['error'],
                'title': '',
                'content': '',
                'tags': '',
                'source_url': url,
                'source_type': 'unknown'
            }
        
        # íƒœê·¸ ìƒì„±
        tags = generate_tags_from_content(result['title'], result['content'])
        
        return {
            'success': True,
            'title': result['title'],
            'content': result['content'],
            'tags': tags,
            'source_url': url,
            'source_type': result['source_type'],
            'original_title': result.get('original_title', ''),
            'summary': result.get('summary', '')
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'title': '',
            'content': '',
            'tags': '',
            'source_url': url,
            'source_type': 'unknown'
        }

def generate_tags_from_content(title: str, content: str) -> str:
    """ì½˜í…ì¸ ì—ì„œ íƒœê·¸ ìƒì„±"""
    try:
        if not openai.api_key:
            return "ë¸”ë¡œê·¸, ì •ë³´, ê°€ì´ë“œ"
        
        prompt = f"""
ë‹¤ìŒ ì½˜í…ì¸ ì— ì í•©í•œ íƒœê·¸ë¥¼ 5-8ê°œ ìƒì„±í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
ë‚´ìš©: {content[:500]}

ìš”êµ¬ì‚¬í•­:
1. ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ íƒœê·¸ ëª©ë¡
2. í•œêµ­ì–´ íƒœê·¸
3. ê´€ë ¨ì„± ë†’ì€ íƒœê·¸
4. ê²€ìƒ‰ì— ìœ ìš©í•œ íƒœê·¸
"""
        
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=200,
            temperature=0.7
        )
        
        tags = response.choices[0].message.content.strip()
        return tags
        
    except Exception as e:
        return "ë¸”ë¡œê·¸, ì •ë³´, ê°€ì´ë“œ" 